{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIJHHdD_LLK5"
   },
   "source": [
    "# Transfer learning and pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "shred -u setup_colab.py\n",
    "\n",
    "wget https://raw.githubusercontent.com/Alenush/mds_nlp_2021/main/utils/setup_colab.py -O setup_colab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setup_colab\n",
    "\n",
    "setup_colab.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in your Coursera token and email\n",
    "To successfully submit your answers to our grader, please fill in your Coursera submission token and email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grading\n",
    "\n",
    "all_parts = [\"tS7BO\", \"6gZAz\", \"TdWnl\", \"0ClQi\", \"b8JOi\", \"LtFTq\", \"X91HC\", \"AW7xi\", \"1DoPa\", \"LapxE\", \"aKlhZ\", \"3aCIW\", \"h2vQy\"]\n",
    "grader = grading.Grader(\n",
    "    assignment_key=\"WMPlZI7kTWOGKLZJMQtFUA\",\n",
    "    all_parts=all_parts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token expires every 30 min\n",
    "COURSERA_TOKEN = \"### YOUR TOKEN HERE ###\"\n",
    "COURSERA_EMAIL = \"### YOUR EMAIL HERE ###\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current week assignment have to prepare the data and tokenize it into subwords, and finally use it as input to some pretrained models, for example BERT. \n",
    "\n",
    "<br>\n",
    "\n",
    "You have to:\n",
    "1. implement BPE algorithm \n",
    "2. use ELMO and compare it with word2vec embeddings\n",
    "3. explore the usage of BERT\n",
    "4. train a classifier using BERT embeddings to solve COLA classification task\n",
    "5. use prepared pipelines\n",
    "\n",
    "<br>\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXXjbXIE0LF_",
    "outputId": "30e69012-6ee5-4fac-b3ac-4fd721b3286c"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import wget\n",
    "import os\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poFVp4kAxxk9"
   },
   "source": [
    "This week we will work with the [The Corpus of Linguistic Acceptability (CoLA)](https://nyu-mll.github.io/CoLA/) dataset. It is a single sentence classification task with sentences labeled as grammatically correct or incorrect. \n",
    "\n",
    "Download the dataset and load the data, preapare train and dev set. In the following tasks use dev set to predict the model score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Downloading dataset...')\n",
    "\n",
    "# The URL for the dataset zip file.\n",
    "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
    "\n",
    "# Download the file (if we haven't already)\n",
    "if not os.path.exists('./cola_public_1.1.zip'):\n",
    "    wget.download(url, './cola_public_1.1.zip')\n",
    "\n",
    "# Unzip the dataset (if we haven't already)\n",
    "if not os.path.exists('./cola_public/'):\n",
    "    !unzip cola_public_1.1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "CiWO2fsFcmnk",
    "outputId": "a7a71ebf-942c-4d56-dc13-3d8752742172"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "df_dev = pd.read_csv(\"./cola_public/raw/in_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "breyN4ONctk5",
    "outputId": "0be50540-3726-4153-d480-21e69026edbb"
   },
   "outputs": [],
   "source": [
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "\n",
    "set(labels), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GorO0Tn9HGGU"
   },
   "outputs": [],
   "source": [
    "sentences_dev = df_dev.sentence.values\n",
    "labels_dev = df_dev.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23L1Bs81OU_K"
   },
   "source": [
    "## BPE algorithm\n",
    "\n",
    "**Task**. Implement the BPE algorithm. \n",
    "\n",
    "In the video we have discussed that for pre-trained models it is better to use subword tokenization. BPE is a commonly used tokenizer.\n",
    "\n",
    "\n",
    "In this task you need to implement the BPE algorithm from scratch.\n",
    "\n",
    "\n",
    "This approach solves OOV problem by encoding rare or unknown words as sequence of subword units.\n",
    "For example, the model sees an out of vocabulary word `Transformer`. Instead of directly using a default unknown token, <unk>, the algorithm uses information about words such as `trans`, `form`, which appear in the corpus, to encode the word `Transformer`. Hence, the model might be able to pick up more information compared with the setting when is uses an unknown token. Thus, any word that does not appear in the training corpus can be broken down into subword units, which appear in the corpus.\n",
    "\n",
    "### Recap the procedure of BPE algo\n",
    "\n",
    "Procedure:\n",
    "\n",
    "1) learn \"BPE rules\", i.e., which pairs of symbols to merge; \n",
    "\n",
    "2) apply learned rules to segment a text.\n",
    "\n",
    "In order to do this we need to perform the following actions:\n",
    "\n",
    "* Get the word count frequency\n",
    "* Get the initial token count and frequency (i.e. how many times each character occurs)\n",
    "* Merge the most common byte pairing\n",
    "* Add this to the list of tokens and recalculate the frequency count for each token; this will change with each merging step\n",
    "* Rinse and repeat this procedure until you have reached a predefined token limit or maximum number of iterations (as in the example)\n",
    "\n",
    "**Task 1. (2 points)** You need to **write three functions**:\n",
    "\n",
    "1. `get_statistics` computes token frequencies dictionary in the vocab. Note: You need to create a list of bigram frequencies from the vocabulary.\n",
    "\n",
    "2. `merge_vocab` creates a merge table from statistics. It take  a `pair` (bigrams) and the current `vocab` as an input. The function outputs a new `vocab`, created from the old one by joining together the characters in the pair, if their union is a word from the dictionary.\n",
    "\n",
    "3. `learn_bpe_rules` takes the current `vocab` word-frequency dictionary and the fraction of the total `vocab_size` to merge characters in the words of the dictionary `num_merges` times. Then for each *merge* operation it `get_stats` the counter for each pair of character sequences. Then, it selects the most frequent pair of symbols and merges this pair of symbols in each word in the `vocab` containing it (this pair).\n",
    "\n",
    "Apply 2000 merge steps and write the frequnecy for the pair ('com', 'p') in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUtoqwJUyQpz"
   },
   "outputs": [],
   "source": [
    "# Let's create a symbols frequnecy vocabulary from the data\n",
    "vocab = defaultdict(int)\n",
    "for sentence in sentences:\n",
    "  toks = nltk.word_tokenize(sentence.lower())\n",
    "  for tok in toks:\n",
    "    vocab[(\" \".join(list(tok))+' </w>')] += 1\n",
    "\n",
    "# check that the word `transformers` are not in the vocabulary\n",
    "assert(vocab[\"t r a n s f o r m e r s </w>\"] == 0)\n",
    "# check that `or` in the vocab\n",
    "assert(vocab[\"o r </w>\"] == 61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sgl-iESK5C-"
   },
   "outputs": [],
   "source": [
    "# EXERCISE 1.1\n",
    "import re, collections\n",
    "\n",
    "def get_statistics(vocab):\n",
    "    # YOUR CODE HERE\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, cur_vocab):\n",
    "    v_out = {}\n",
    "    # YOUR CODE HERE\n",
    "    return v_out\n",
    "\n",
    "def learn_bpe_rules(vocab, num_merges=10):\n",
    "    bpe_codes = {}\n",
    "    # YOUR CODE HERE\n",
    "    # use previous functions: get_statistics and merge_vocab to learn bpe rules\n",
    "    return vocab, bpe_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMn_wPIrK5FR"
   },
   "outputs": [],
   "source": [
    "pair_stats = get_statistics(vocab)\n",
    "pair_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ys2Vj257sQwK"
   },
   "outputs": [],
   "source": [
    "best_pair = max(pair_stats, key=pair_stats.get)\n",
    "print(best_pair)\n",
    "\n",
    "new_vocab = merge_vocab(best_pair, vocab)\n",
    "new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BbE0eS23sfNS",
    "outputId": "324be721-0eda-4799-a2e9-447531987bea"
   },
   "outputs": [],
   "source": [
    "# set num_merges to 2.000\n",
    "vocab, bpe_rules = learn_bpe_rules(vocab, num_merges=2000)\n",
    "bpe_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = ## YOUR FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3S6s5H4vNp8z"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(all_parts[0], frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vPXwdosuXcv"
   },
   "source": [
    "**Task 1.2. (1 point)** After we have learned the vocabulary we need to apply the rules in the merge table to new words in the vocabulary. As a final answer write the subwords of the word `transformers` separated by comma. For example: `\"tr,an,sf,or,me,rs\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyWi0Mk3vOYw"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def get_pairs(word):\n",
    "    # YOUR CODE HERE\n",
    "    return pairs\n",
    "\n",
    "def create_new_word(word, pair_to_merge):\n",
    "    # YOUR CODE HERE\n",
    "    return new_word\n",
    "\n",
    "def encode(original_word, bpe_rules):\n",
    "    # YOUR CODE HERE\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJ2vtv2QvfLy",
    "outputId": "4f8aec42-92d7-4b0b-b862-9c69df3bcaf6"
   },
   "outputs": [],
   "source": [
    "original_word = 'transformers'\n",
    "encode(original_word, bpe_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subwords = '' ## YOUR SUBWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBwfcw5vU-JZ"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(all_parts[1], subwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIoFJlBAOXkE"
   },
   "source": [
    "## ELMO embeddings\n",
    "\n",
    "In this part we will see how the ELMO model can be loaded and will check its embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCPNWfxnK5Hi"
   },
   "outputs": [],
   "source": [
    "# If you work in Colab, for correct work, please, \n",
    "# use the following versions (and reinstall colab after): \n",
    "\n",
    "!pip install tensorflow==1.15\n",
    "!pip install \"tensorflow_hub>=0.6.0\"\n",
    "!pip3 install tensorflow_text==1.15\n",
    "\n",
    "# You can download ELMO models from here: https://allennlp.org/elmo\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\")\n",
    "elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbLpSHpD8NSM",
    "outputId": "96866525-119d-46fa-ad3a-f2091ec6c3bb"
   },
   "outputs": [],
   "source": [
    "# first, you need to initialise the session to get embeddings:\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdKsaKQ8Q0rP",
    "outputId": "291e3285-0eee-422d-b878-7cd6781f690f"
   },
   "outputs": [],
   "source": [
    "# You can get ELMO embeddings feeding it text string and use default signature or tokenized text.\n",
    "embeddings = elmo([\"Some input text\", \n",
    "                   \"Test sentence\"],\n",
    "             signature=\"default\",\n",
    "             as_dict=True)[\"elmo\"]\n",
    "\n",
    "print(\"text\")\n",
    "emb_text = sess.run(embeddings[0][2])\n",
    "emb_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4agig6di_zU"
   },
   "source": [
    "**Task 2 (2 points)**. You have texts with several sentences where the word `bank` is used in different contexts. Load the word2vec model `word2vec-google-news-300` and elmo model from `https://tfhub.dev/google/elmo/2`. \n",
    "Check the embedding of the word `bank` in each sentence. \n",
    "Compare word2vec and ELMO embeddings.\n",
    "\n",
    "Write the cosine similarity distance between the embeddings of the word bank in the first sentence and in the third sentence for both word2vec and ELMO. Round the answer up to two digits after the decimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IsgrG-zjjJi_",
    "outputId": "506e0aa2-71a7-4c30-cca9-cea89c37c4f6"
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "         \"The river bank was flooded\", # this one\n",
    "         \"The bank vault was robust\",\n",
    "         \"He had to bank on her for support\", # the third one\n",
    "         \"The bank was out of money\",\n",
    "         \"The robber still the money from the bank\"\n",
    "]\n",
    "\n",
    "tokenized_input = []\n",
    "tokens_length = []\n",
    "for sentence in texts:\n",
    "    toks_sent = nltk.word_tokenize(sentence.lower())\n",
    "    tokenized_input.append(toks_sent)\n",
    "    tokens_length.append(len(toks_sent))\n",
    "\n",
    "print(len(tokenized_input), len(tokens_length))\n",
    "tokenized_input, tokens_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLNvnjD1ShKe",
    "outputId": "14b3a758-20fe-4f2d-e45d-e92d0c7aee83"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE FOR WORD2VEC HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSMZQduNdtpI",
    "outputId": "df646468-19b1-4ad2-e289-83603b0572f9"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE FOR ELMO HERE \n",
    "# YOU need pad for signature=\"tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7FCmaLEUEQJ",
    "outputId": "ad224796-087c-4ee0-e67a-7653d27a876b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhUdbZdvT6iI"
   },
   "outputs": [],
   "source": [
    "cosine_similarity_word2vec = ## YOUR COSINE SIMILARITY FOR Word2vec\n",
    "cosine_similarity_elmo = ## YOUR COSINE SIMILARITY FOR ELMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(all_parts[2], cosine_similarity_word2vec)\n",
    "grader.set_answer(all_parts[3], cosine_similarity_elmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h004MhBtOaNy"
   },
   "source": [
    "## BERT embeddings\n",
    "\n",
    "Next, we will work with BERT models. We will work with **transformers** library. It's the most commonly used library for working with pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZi5VS1eUvT5",
    "outputId": "880f7d18-e66f-4dae-a191-76da0a058269"
   },
   "outputs": [],
   "source": [
    "# If you work in Colab:\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cX2ggGVmypO"
   },
   "source": [
    "You will work with English BERT uncased model. First, you need to load the tokenizer and the pretrained model. The full list of models available you can find [here](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuEp9yE5U0cf",
    "outputId": "a97fe885-ffa8-4145-f0d2-5401c3c318d9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfiY5i7znAeZ"
   },
   "source": [
    "**Task 3.1. (0.5 point)** Write the function for input preparation for the BERT model. \n",
    "\n",
    "For this you need:\n",
    "1. Split each sentence and tokenize it with the tokenizer initialized above.\n",
    "2. Add special tokens `[CLS]` and `[SEP]`.\n",
    "3. Map all tokens to their IDs.\n",
    "4. Pad or truncate all sentences to the same length.\n",
    "5. Create attention masks which explicitly differentiate real tokens from `[PAD]` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9mIB0isXX_oD",
    "outputId": "efa07946-a6ea-4cf5-90de-1038f257dc47"
   },
   "outputs": [],
   "source": [
    "# WRITE function for preprocessing input for BERT\n",
    "# Write the sum of input_ids of the `Attention is all you need.`\n",
    "\n",
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    OUTPUT:  input_ids': tensor([[ 101, 3086, 2003, 2035, 2017, 2342, 1012,  102]]), \n",
    "    'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), \n",
    "    'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
    "    (['[CLS]', 'attention', 'is', 'all', 'you', 'need', '.', '[SEP]'],\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return tokenized_text, input_ids, token_type_ids, attention_mask\n",
    "\n",
    "# Write in the answer the input ids of the sequence:\n",
    "text = \"Attention is all you need.\"\n",
    "tokenized_text, input_ids, token_type_ids, attention_mask = bert_text_preparation(text, tokenizer)\n",
    "print(tokenized_text)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_sum = ## YOUR INPUT IDs SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Avqj_E31o2LN"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(all_parts[4], input_ids_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcIXMVkbaqY-"
   },
   "source": [
    "**Task 3.2. (1 point)** Apply `BertForMaskedLM` model. `BertForMaskedLM` is a Language Model head on top of BERT. It maps the hidden state output of BERT model to a specific token in the vocabulary. The loss is calculated based on the scores obtained from a given token with respect to the target token. We can `[MASK]` some tokens in the text and force BERT to predict the masked words. That is exactly what you need to do. \n",
    "\n",
    "Write a function that masks an indexed word and predicts it. Write the predicted index and the corresponding token.\n",
    "\n",
    "**Note:** pass a vector of `1`s as `token_type_ids` to encode the token type (that means that as long as you have only one segment than all tokens should have the same type id, use id `1` for the correct grading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_N-pTAWYKlF",
    "outputId": "bbb42819-bc98-4862-d2ef-923cfa53488b"
   },
   "outputs": [],
   "source": [
    "# EXERCISE 3.2. \n",
    "# Write function to predict MASKED token.\n",
    "\n",
    "def find_masked_word(text, masked_index):\n",
    "  \"\"\" \n",
    "  Tokenize input - don't forget to add [CLS] ans [SEP] tokens.\n",
    "  Mask the word with the `masked_index`\n",
    "  use `model` BertForMaskedLM above to predict the MASKED token.\n",
    "  Return: predicted_index\n",
    "  \"\"\"\n",
    "  # YOUR CODE HERE\n",
    "  return predicted_index\n",
    "\n",
    "# Write\n",
    "text = \"What is the best pretrained model? [SEP] BERT think that it is!\"\n",
    "predicted_index = find_masked_word(text, masked_index=14)\n",
    "print(predicted_index)\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(all_parts[5], predicted_index)\n",
    "grader.set_answer(all_parts[6], predicted_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Vonv-duiSTU"
   },
   "source": [
    "**Task 3.3. (1 point)**. Get token and sentence embeddings from BERT. Now, load pretrained `BertModel`. We know how to preprocess text for BERT. Next, we need to get embeddings from the model. \n",
    "\n",
    "First, write a function that gets  tokens_tensors of a sentence, segment ids, and model as an input. It outputs the `list_token_embeddings` - the embeddings of this sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5bec1f8c4Fd",
    "outputId": "6fe66ebb-8aa5-4d79-ed95-a8d03e2ba1a3"
   },
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "bert_model = bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3Dm_ECSYkuv"
   },
   "outputs": [],
   "source": [
    "# EXERCISE 3.3. Write function that gets the BERT embedding directly\n",
    "\n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns: [n_tokens, n_embedding_dimensions]\n",
    "    containing embeddings for each token\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "\n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAs40gj3YngV"
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "         \"The river bank was flooded.\",\n",
    "         \"The bank vault was robust.\",\n",
    "         \"He had to bank on her for support.\",\n",
    "         \"The bank was out of money.\",\n",
    "         \"The robber still the money from the bank.\"\n",
    "]\n",
    "\n",
    "# Getting embeddings for the target sentences\n",
    "# Here we put the word `bank` in all given contexts\n",
    "target_word_embeddings = []\n",
    "\n",
    "for text in texts:\n",
    "    tokenized_text, input_ids, token_type_ids, attention_mask = bert_text_preparation(text, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(input_ids, attention_mask, bert_model)\n",
    "    # Find the position 'bank' in list of tokens\n",
    "    word_index = tokenized_text.index('bank')\n",
    "    # Get the embedding for bank\n",
    "    word_embedding = list_token_embeddings[word_index]\n",
    "    target_word_embeddings.append(word_embedding)\n",
    "\n",
    "assert(len(target_word_embeddings) == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeOwblLmvWRq"
   },
   "source": [
    "Use the cosine_similarity metrics between embeddings and calculate it for the first and third sentences (sentence indexes 0 and 2), where the meaning of the `bank` words are different; and the sentences 4 and 5, where their senses are similar. Round the answer up to 3 digits after the decimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkC6fNPfrMBf",
    "outputId": "681ea479-c920-426d-b876-02c314fcfe0d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "## Cosine similarity [different meanings]\n",
    "cosine_similarity_diff = ## YOUR ANSWER\n",
    "\n",
    "## Cosine similarity [similar meanings]\n",
    "cosine_similarity_sim = ## YOUR ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(all_parts[7], cosine_similarity_diff)\n",
    "grader.set_answer(all_parts[8], cosine_similarity_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NREP4ks8x90n"
   },
   "source": [
    "**Task 3.4 (1 point)**. Write a function that recieves a sentence embedding from the BERT model. Compute sentence embeddings for all the sentences and write the cosine similarity between the first and third sentences and between the 4th and the 5th. Round the answer up to 3 digits after the decimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_LeBgVbrqjh",
    "outputId": "e37f6adf-2285-44f0-d5d0-9bf9964a9c41"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sentence_embedding(text, bert_model):\n",
    "    # YOUR CODE HERE\n",
    "    return sentence_embedding[:, 0, :].cpu().numpy()\n",
    "\n",
    "sent_embeddings = []\n",
    "for text in texts:\n",
    "    sentence_embedding = get_sentence_embedding(text, bert_model)\n",
    "    sent_embeddings.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cosine similarity  between the 1st and the 3rd sentences\n",
    "cosine_similarity_1_3 = ## YOUR ANSWER\n",
    "\n",
    "## Cosine similarity  between the 4th and the 5th sentences\n",
    "cosine_similarity_4_5 = ## YOUR ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(all_parts[9], cosine_similarity_1_3)\n",
    "grader.set_answer(all_parts[10], cosine_similarity_4_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNPDy15JiqC4"
   },
   "source": [
    "**Task 3.5. (1 point)**. Using sentence embeddings apply a simple classifier for the COLA dataset you loaded in the begining of this notebook. Use SVM with hyperparameters `(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=42)` from `sklearn` library. Do not change random seed and parameters! Write accuracy in percentages for the validation set. For example, 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBdItMI2fOGZ",
    "outputId": "f01ecee5-9a87-44f7-e317-d8f788ab988f"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# YOUR CODE HERE\n",
    "accuracy_in_percentages = ## YOUR ACCURACY HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6gX-VVZgJL1",
    "outputId": "29838c4c-984f-4ac1-8767-16b0841c7243"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(all_parts[11], accuracy_in_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlXtVbTNpAeJ"
   },
   "source": [
    "**Task 4 (0.5 point)**.  In the hugging face library you can use pipelines that are already prepared for you. Import `pipilene` and choose the task you need to solve, the model from the huggingface list and tokenizer, like in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRux8Qp2hkXr"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Using default model and tokenizer for the task\n",
    "pipeline(\"<task-name>\")\n",
    "\n",
    "# # Using a user-specified model\n",
    "pipeline(\"<task-name>\", model=\"<model_name>\")\n",
    "\n",
    "# # Using custom model/tokenizer as str\n",
    "pipeline('<task-name>', model='<model name>', tokenizer='<tokenizer_name>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aI74xl41FNl1"
   },
   "source": [
    "Write the pipeline for the Question answering task using the `distilbert-base-uncased-distilled-squad` model. It's distilled BERT model that was fine-tuned on QA dataset `SQUAD`. Write the start id of the answer, using as a context 1000 sentences from COLA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf57MRzSiSON"
   },
   "outputs": [],
   "source": [
    "context = \"\\n\".join(sentences[:1000])\n",
    "question = \"When does Mary get depressed?\"\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzPdp-mWGTfv"
   },
   "outputs": [],
   "source": [
    "start_id = ## YOUR START ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(all_parts[12], start_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗️Remember to **run the last code cell** to submit the solution."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week5 Programming assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
